{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "085e9cf3-7e68-4279-ba32-285e9a550acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Je', \"n'\", 'aime', 'pas', 'le', 'lait'], 'pos': ['PRON', 'NEG', 'VERB', 'ADV', 'DET', 'NOUN']}\n",
      "{'tokens': ['Tu', 'adores', 'le', 'chocolat'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'temps passé 11h'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "import json\n",
    "goalpos = json.load(open(\"sequoia.test.json\", \"r\"))\n",
    "\n",
    "\n",
    "# Charger le modèle français\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    \n",
    "def predict_pos(sentence, model):\n",
    "    model.tokenizer = lambda x: Doc(model.vocab, x.split())\n",
    "    return [token.pos_ for token in model(sentence)]\n",
    "    \n",
    "#print(predict_pos(\"J' aime le chocolat\", nlp))\n",
    "\"\"\" \n",
    "1. What is lambda function in python and why do we use such a function in line 4 ? \n",
    "    Réponse: Lambda is an anonyme function that turns the sentence X into tokens with split function on spaces (\" \")\n",
    "    it returns a DOC object with the tokenized sentence. It avoids using a more complexe tokenizer this one onlys split on spaces\n",
    "2. Explain the last line of the predict_pos function.\n",
    "    Réponse: this part:\" model(sentence)\" calls the simplified tokenizer with lambda function on sentence and it returns a DOc object with \n",
    "    the tokenized sentence\n",
    "    this part:\"for token in model(sentence)\" boucle on each word in the sentence\n",
    "    this part\"token.pos_ \"associates a tag to the each token\n",
    "    thepredict_pos  returns for the setence \"J' aime le chocolat this list:['PRON', 'VERB', 'DET', 'NOUN']\n",
    "3. Write a function, that takes as input a list of sentences and returns a list of PoS annotations\n",
    " (i.e. a list of lists of PoS)\n",
    " \"\"\"\n",
    "def predict_pos_list(list_sentence, model):\n",
    "    # cut the minu corous on., ?, !, ou ...\n",
    "    sentences = re.split(r'[.!?]+', list_sentence)\n",
    "    # clean the sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip() != \"\"]\n",
    "    list_pos=[]# to store list of lists of PoS\n",
    "    for l in sentences:\n",
    "        list_pos.append(predict_pos(l, model))# calls predict_pos on a single sentence to return the list of POS\n",
    "    return list_pos# return list of lists of PoS\n",
    "\n",
    "        \n",
    "#print(predict_pos_list(\"J' aime le chocolat. Je n'aime pas le lait\", nlp))\n",
    "\n",
    "\"\"\"\n",
    " 4. Whydowe(have to) consider tokenized sentences to evaluate a PoS tagger?\n",
    " because the POS tagger gives words' labels not sentences'labels for example usually a  sentence is not a DET except one word sentences.\n",
    " 5. Implement each of the evaluation metric defined in this section.\"\"\"\n",
    "\"\"\"\n",
    "Create a dico where pos labels are keys and words value\n",
    "sentence_accuracy create a vector if 1 value of the vector is Zero it returns 0 or if the length is different\n",
    "micro_word_accuracy create a vector and calculate sum(vector)/ number of words\n",
    "macro_word_accuracy creates a dictionnary with postags and their occurences in the list of poses only if the vecto indicates it is accurate\n",
    "\"\"\"\n",
    "\"\"\"refaire toutes les fonctions pour que dico soit de la forme{'tokens': ['Je', \"n'\", 'aime', 'pas', 'le', 'lait'], 'pos': ['PRON', 'NEG', 'VERB', 'ADV', 'DET', 'NOUN']}\n",
    "{'tokens': ['Tu', 'adores', 'le', 'chocolat'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']}\"\"\"\n",
    "def sentence_accuracy(poslist,dico):\n",
    "    vector=[]\n",
    "    accuracy=0\n",
    "    lengthposlit=len(poslist)\n",
    "    lengthdico=len(dico)\n",
    "    minlength=min(lengthdico, lengthposlit)\n",
    "    compteur=0\n",
    "    for k in dico:\n",
    "        if compteur<minlength:\n",
    "            if poslist[compteur]==k and poslist[compteur] in dico:\n",
    "                vector.append(1)\n",
    "                compteur=compteur+1\n",
    "            else:\n",
    "                vector.append(0)\n",
    "                compteur=compteur+1\n",
    "    if lengthposlit>lengthdico:# adds 0 to the vector if poslit longueur than dico\n",
    "        for j  in range(compteur,lengthposlit-1):\n",
    "            vector.append(0)\n",
    "    if sum(vector)==lengthdico:\n",
    "        accuracy=1\n",
    "    else:\n",
    "        accuracy=0\n",
    "    return vector, accuracy\n",
    "    \n",
    "def micro_word_accuracy(poslist,dico):\n",
    "    vector= sentence_accuracy(poslist,dico)[0]\n",
    "    word_accuracy=sum(vector)\n",
    "\n",
    "    return int(word_accuracy)\n",
    "def macro_word_accuracy(poslist,dico,dico_poslabels):# has alreday  dico_poslabels as an argyment to enrich it\n",
    "    vector= sentence_accuracy(poslist,dico)[0]\n",
    "    lendico=len(dico)\n",
    "    lenvector=len(vector)\n",
    "    lenmin=min(lenvector,lendico)\n",
    "    compteur=0\n",
    "    for k in dico:\n",
    "        if compteur<lenmin:\n",
    "            if k in dico_poslabels and vector[compteur]==1:#if alreaddy in dico that adds 1\n",
    "                dico_poslabels[k]=dico_poslabels[k]+1\n",
    "            if k not in dico_poslabels and vector[compteur]==1:#if not in dico creates a key\n",
    "                dico_poslabels[k]=1\n",
    "            if k not in dico_poslabels and vector[compteur]==0:#if not in dico creates a key creates a key with value 0\n",
    "                dico_poslabels[k]=0 \n",
    "        else:\n",
    "            dico_poslabels[k]=0# if the list of pos labels is longueur than the list of gold labels\n",
    "            \n",
    "        compteur=compteur+1\n",
    "\n",
    "    return dico_poslabels\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def postagger_accuracy(poslist_list, dico):\n",
    "    sentence=0\n",
    "    proportion=0\n",
    "    lengthtexte=0\n",
    "    correctpos={}\n",
    "    \n",
    "    for i in range(len(poslist_list)-1):\n",
    "        sentence=sentence+sentence_accuracy(poslist_list[i],dico[i])[1]\n",
    "        proportion= proportion+micro_word_accuracy(poslist_list[i],dico[i])\n",
    "        lengthtexte=lengthtexte+len(dico[i])\n",
    "        correctpos=macro_word_accuracy(poslist_list[i],dico[i],correctpos)#update the correctpos by adding new keys if necessary or updating the key value\n",
    "    proportion=(proportion/lengthtexte)*100\n",
    "    return sentence,proportion,correctpos\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "s = [\"Je\", \"n'\", \"aime\", \"pas\", \"le\", \"lait\"]\n",
    "posgoal = ['PRON','NEG','VERB', 'ADV', 'DET', 'NOUN']\n",
    "x=[\n",
    "    [\"Je\", \"n'\", \"aime\", \"pas\", \"le\", \"lait\"],\n",
    "    [\"Tu\", \"adores\", \"le\", \"chocolat\"],\n",
    "    [\"Il\", \"mange\", \"une\", \"pomme\"],\n",
    "    [\"Nous\", \"regardons\", \"la\", \"télévision\"],\n",
    "    [\"Elle\", \"ne\", \"parle\", \"jamais\", \"aux\", \"étrangers\"],\n",
    "    [\"Ils\", \"aiment\", \"les\", \"voyages\"],\n",
    "    [\"Je\", \"lis\", \"un\", \"livre\"],\n",
    "    [\"Tu\", \"n'\", \"écoutes\", \"pas\", \"le\", \"professeur\"],\n",
    "    [\"Elle\", \"porte\", \"une\", \"robe\"],\n",
    "    [\"Nous\", \"habitons\", \"à\", \"Paris\"]\n",
    "]\n",
    "\n",
    "y=[\n",
    "    ['PRON', 'NEG', 'VERB', 'ADV', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'NEG', 'VERB', 'ADV', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'NEG', 'VERB', 'ADV', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ['PRON', 'VERB', 'PREP', 'NOUN']\n",
    "]\n",
    "\n",
    "texte=\"\"\"Je n' aime pas le lait.\n",
    "Tu adores le chocolat.\n",
    "Il mange une pomme.\n",
    "Nous regardons la télévision.\n",
    "Elle ne parle jamais aux étrangers.\n",
    "Ils aiment les voyages.\n",
    "Je lis un livre.\n",
    "Tu n' écoutes pas le professeur.\n",
    "Elle porte une robe.\n",
    "Nous habitons à Paris.\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"test on one sentence:\n",
    "smalldico=dict(zip(posgoal,s))\n",
    "dico_poslabels={}\n",
    "test=['PRON', 'VERB', 'DET', 'NOUN']\n",
    "print(macro_word_accuracy(test,smalldico,dico_poslabels))\"\"\"\n",
    "# test on list of list\n",
    "goldpos = [{\"tokens\": tokens, \"pos\": tags} for tokens, tags in zip(x, y)]\n",
    "\n",
    "# Vérification\n",
    "for phrase in goldpos[:2]:\n",
    "    print(phrase)\n",
    "lpos_texte=predict_pos_list(texte, nlp)\n",
    "#print(postagger_accuracy(lpos_texte, dictionnaires ))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"6. How can these new metrics be computed easily (i.e. by using the functions you have\n",
    " already implemented)\n",
    " They can be easily implemented because they coorrespond to the number of sentences in the text minus sentence_accuracy[1] or the numbers of words in the text minus\n",
    " micro_word_accuracy\n",
    "\"\"\"\n",
    "def word_error_rate(poslist_list, dico):\n",
    "    word_accurate=0\n",
    "    lengthtexte=0\n",
    "    for i in range (len(dico)-1):\n",
    "        word_accurate=word_accurate+micro_word_accuracy(poslist_list[i],dico[i])\n",
    "        lengthtexte=lengthtexte+len(dico[i])\n",
    "    return ((lengthtexte-word_accurate)/lengthtexte)*100\n",
    "\n",
    "    \n",
    "#print(word_error_rate(lpos_texte, dictionnaires ))\n",
    "\n",
    "\n",
    "def corpus_stats(filepath):\n",
    "    \"\"\"\n",
    "    Affiche le nombre de phrases et de mots dans un corpus JSON\n",
    "    (liste de dictionnaires avec clés 'tokens' et 'pos' ou 'goldpos').\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = json.load(f)\n",
    "\n",
    "    # Nombre de lignes = nombre d'entrées dans la liste\n",
    "    nb_lignes = len(corpus)\n",
    "    nb_mots=0 # initialize varaible at 0\n",
    "    listkey=[]\n",
    "    i=0\n",
    "    while i<len(corpus):# this whole loop must be changed\n",
    "        for k in corpus[i]:\n",
    "            listemots=corpus[i][k]\n",
    "            nb_mots=nb_mots+len(listemots)# not efficient becuase count pos and it is useless but works\n",
    "        i=i+1\n",
    "    nb_mots=int(nb_mots/2) # divides by 2 to retrieve the Pos labels of the number of words\n",
    "    \n",
    "    print(f\"Nombre de phrases : {nb_lignes}\")\n",
    "    print(f\"Nombre total de mots : {nb_mots}\")\n",
    "\n",
    "    return nb_lignes, nb_mots\n",
    "#print(corpus_stats(\"sequoia.test.json\"))\n",
    "\"\"\" 8. Evaluate the performance achieved by the tagger described in Section 2 on the corpus.\n",
    " Interpret.\"\"\"\n",
    "def corpus_tokens_only(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = json.load(f)\n",
    "    liste_phrase=[]\n",
    "    i=0\n",
    "    while i<len(corpus):# this whole loop must be changed\n",
    "        for k in corpus[i]:\n",
    "            if k==\"tokens\":\n",
    "                liste_phrase.append(\" \".join(corpus[i][k]))\n",
    "        i=i+1\n",
    "    str_corpus=\"\".join(liste_phrase)\n",
    "    return str_corpus\n",
    "       \n",
    "                \n",
    "corpusbis=corpus_tokens_only(\"sequoia.test.json\")# list of tokenize sentences from the sequoia file\n",
    "lpos=predict_pos_list(corpusbis, nlp)\n",
    "\"\"\"print(\"lpos\")\n",
    "print(lpos)\n",
    "print(\"goalpos\")\n",
    "print(goalpos)\"\"\"\n",
    "\n",
    "\"\"\"temps passé 11h\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee185f-6e32-4069-9094-8bf17c64e1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scikit-learn)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
