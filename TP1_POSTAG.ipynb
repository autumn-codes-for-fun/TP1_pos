{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e9cf3-7e68-4279-ba32-285e9a550acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test function predict_pos\n",
      "Phrase : Je mange une pomme\n",
      "Tags prédits : ['PRON', 'VERB', 'DET', 'NOUN']\n",
      "Test function predict_pos_list:\n",
      "[['PRON', 'VERB', 'DET', 'NOUN'], ['DET', 'NOUN', 'ADP', 'NOUN']]\n",
      "Test function  sentence_accuracy:\n",
      "([1, 1, 0], 0)\n",
      " test de WER sur le courpus sequoia:\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "import json\n",
    "goalpos = json.load(open(\"sequoia.test.json\", \"r\"))\n",
    "\n",
    "# Charge the french model from spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "#function given in the exercice by the teacher   \n",
    "def predict_pos(sentence, model):\n",
    "    \"\"\"\n",
    "    This function modify the tokenizer included in spacy for a simpler one. It tokenizes a sentence and splits on spaces\n",
    "    Args:\n",
    "    sentence(str): The sentences that will be tokenized and labelled\n",
    "    model (spacy.language.Language):the spacy model charged with fr_core_news_sm\"\n",
    "    \n",
    "    Returns:\n",
    "    list[str]: that cotains the predicted pos for each word\n",
    "\n",
    "    Exemple:\n",
    "     >>> predict_pos(\"Je mange une pomme\", nlp)\n",
    "        ['PRON', 'VERB', 'DET', 'NOUN']\n",
    "    \"\"\"\n",
    "    model.tokenizer = lambda x: Doc(model.vocab, x.split())\n",
    "    return [token.pos_ for token in model(sentence)]\n",
    "    \n",
    "\n",
    "print(\"Test function predict_pos\")\n",
    "sentence = \"Je mange une pomme\"\n",
    "pos_tags = predict_pos(sentence, nlp)\n",
    "print(\"Phrase :\", sentence)\n",
    "print(\"Tags prédits :\", pos_tags)\n",
    "\n",
    "# reponse question 3 du tp\n",
    "def predict_pos_list(list_sentence, model):\n",
    "    \"\"\"\n",
    "    This function cuts a list of sentences on different separators usisng Regex it then iterates  on  the list of sentences and creates a\n",
    "    list of lists of POS\n",
    "    Args:\n",
    "    text [str]: a liste of sentences that will be tokenized and tagued with POS\n",
    "    model (spacy.language.Language):the spacy model charged with fr_core_news_sm\"\n",
    "\n",
    "    Returns:\n",
    "    list[list[str]]]: a list of list of POS \n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]+', list_sentence)\n",
    "    sentences = [s.strip() for s in sentences if s.strip() != \"\"]\n",
    "    list_pos=[]\n",
    "    for l in sentences:\n",
    "        list_pos.append(predict_pos(l, model))\n",
    "    return list_pos\n",
    "\n",
    "print(\"Test function predict_pos_list:\")\n",
    "        \n",
    "texte = \"Je mange une pomme. Tu bois du café !\"\n",
    "print(predict_pos_list(texte, nlp))\n",
    "\n",
    "\n",
    "def sentence_accuracy(poslist,dico):\n",
    "    \"\"\"Compare les étiquettes morphosyntaxiques prédites (PoS) d'une phrase à celles\n",
    "    du jeu de données de référence (gold standard) et calcule la précision\n",
    "    phrase par phrase.\n",
    "\n",
    "    Compares the POS labesl of a sentence to its goldpos (rigth POS labesl) it calculates a vector  if all predict POS match the goldpos\n",
    "    the vector contains only 1 and the sentence accuracy is 1 (True) if one POS is unmatched the accuracy is O (FALSE)\n",
    "\n",
    "    Args:\n",
    "        poslist (list[str]): POS labels predicted by the tagger\n",
    "                             (ex : ['PRON', 'VERB', 'DET', 'NOUN']).\n",
    "        dico (dict): a dictionnary that cotains tokens and their goldpos with tokens and POS as key {'tokens': [...], 'pos': [...]}.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - vector (list[int]): binary list :\n",
    "                * 1 id predict POS and goldpos matches\n",
    "                * 0 if unmatched.\n",
    "            - accuracy (int): 1 all predict POS match goldpos, else 0 .\"\"\"\n",
    "    vector=[]\n",
    "    accuracy=0\n",
    "    lengthposlit=len(poslist)\n",
    "    goldpos=dico.get(\"pos\")\n",
    "    lengthgoldpos=len(goldpos)\n",
    "    minlength=min(lengthposlit, lengthgoldpos)\n",
    "    compteur=0\n",
    "    while compteur<minlength:\n",
    "            if poslist[compteur]==goldpos[compteur]:\n",
    "                vector.append(1)\n",
    "                compteur=compteur+1\n",
    "            else:\n",
    "                vector.append(0)\n",
    "                compteur=compteur+1\n",
    "    if lengthposlit>lengthgoldpos:# adds 0 to the vector if poslit longueur than dico\n",
    "        for i  in range(compteur,lengthposlit):\n",
    "            vector.append(0)\n",
    "    if sum(vector)==lengthgoldpos:\n",
    "        accuracy=1\n",
    "    else:\n",
    "        accuracy=0\n",
    "    return vector, accuracy\n",
    "print(\"Test function  sentence_accuracy:\")\n",
    "pos_pred = ['PRON', 'VERB', 'ADV']\n",
    "gold_data = {'tokens': ['cela', 'signifie', 'que'],'pos': ['PRON', 'VERB', 'SCONJ']}\n",
    "print(sentence_accuracy(pos_pred, gold_data))\n",
    "\n",
    "def micro_word_accuracy(poslist,dico):\n",
    "    \"\"\" Computes the number of correctly predicted PoS (Part-of-Speech) tags \n",
    "    for a given sentence (micro-level accuracy).\n",
    "\n",
    "    This function uses the binary vector returned by `sentence_accuracy()` \n",
    "    to count how many individual word-level tags were correctly predicted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    poslist : list[str]\n",
    "        The list of predicted PoS tags (e.g., ['PRON', 'VERB', 'DET', 'NOUN']).\n",
    "\n",
    "    dico : dict\n",
    "        A dictionary containing the reference (gold standard) sentence and its \n",
    "        corresponding PoS tags. Must have the structure:\n",
    "        {\n",
    "            \"tokens\": [...],\n",
    "            \"pos\": [...]\n",
    "        }\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of correctly predicted PoS tags in the sentence.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> pos_pred = ['PRON', 'VERB', 'ADV']\n",
    "    >>> gold_data = {'tokens': ['cela', 'signifie', 'que'],\n",
    "    ...              'pos': ['PRON', 'VERB', 'SCONJ']}\n",
    "    >>> micro_word_accuracy(pos_pred, gold_data)\n",
    "    2\n",
    "\n",
    "    Explanation\n",
    "    -----------\n",
    "    Out of the 3 tags, 2 are correct ('PRON' and 'VERB'), \n",
    "    so the function returns 2.\n",
    "     \"\"\"\n",
    "    vector= sentence_accuracy(poslist,dico)[0]\n",
    "    word_accuracy=sum(vector)\n",
    "    return int(word_accuracy)  \n",
    "\n",
    "\n",
    "def macro_word_accuracy(poslist,dico,dico_poslabels):\n",
    "    \"\"\"\n",
    "    Computes the number of correctly predicted PoS tags per category (macro-level accuracy).\n",
    "\n",
    "    This function updates a dictionary (`dico_poslabels`) that tracks, \n",
    "    for each PoS tag (e.g., 'VERB', 'NOUN', etc.), how many times that \n",
    "    tag was correctly predicted across all sentences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    poslist : list[str]\n",
    "        The list of predicted PoS tags for one sentence.\n",
    "\n",
    "    dico : dict\n",
    "        A dictionary containing the reference sentence and its PoS tags:\n",
    "        {\n",
    "            \"tokens\": [...],\n",
    "            \"pos\": [...]\n",
    "        }\n",
    "\n",
    "    dico_poslabels : dict\n",
    "        A cumulative dictionary that keeps count of correctly predicted \n",
    "        tags per PoS label. It is updated in-place and returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated dictionary containing the number of correct predictions per PoS label.\n",
    "           Example\n",
    "    -------\n",
    "    >>> pos_pred = ['PRON', 'VERB', 'ADV']\n",
    "    >>> gold_data = {'tokens': ['cela', 'signifie', 'que'],\n",
    "    ...              'pos': ['PRON', 'VERB', 'SCONJ']}\n",
    "    >>> macro_word_accuracy(pos_pred, gold_data, {})\n",
    "    {'PRON': 1, 'VERB': 1, 'SCONJ': 0}\n",
    "\n",
    "    Explanation\n",
    "    -----------\n",
    "    - 'PRON' is correct → count = 1  \n",
    "    - 'VERB' is correct → count = 1  \n",
    "    - 'SCONJ' was mispredicted → count = 0\"\"\"\n",
    "    vector= sentence_accuracy(poslist,dico)[0]\n",
    "    goldpos=dico.get(\"pos\")\n",
    "    lengthgoldpos=len(goldpos)\n",
    "    lenvector=len(vector)\n",
    "    lenmin=min(lenvector,lengthgoldpos)\n",
    "    compteur=0\n",
    "    while compteur<lenmin:\n",
    "        if goldpos[compteur] in dico_poslabels and vector[compteur]==1:#if alreaddy in dico that adds 1\n",
    "                dico_poslabels[goldpos[compteur]]=dico_poslabels[goldpos[compteur]]+1\n",
    "        if goldpos[compteur] not in dico_poslabels and vector[compteur]==1:#if not in dico creates a key\n",
    "            dico_poslabels[goldpos[compteur]]=1\n",
    "        if goldpos[compteur] not in dico_poslabels and vector[compteur]==0:#if not in dico creates a key creates a key with value 0\n",
    "            dico_poslabels[goldpos[compteur]]=0             \n",
    "        compteur=compteur+1\n",
    "    #add pos labels that have not been predicted at all\n",
    "    for compteur  in range(compteur,lengthgoldpos):\n",
    "        if goldpos[compteur] not in dico_poslabels:\n",
    "            dico_poslabels[goldpos[compteur]]=0  \n",
    "    \n",
    "    return dico_poslabels\n",
    "        \n",
    "\n",
    "def postagger_accuracy(poslist_list, dico):\n",
    "    \"\"\"\n",
    "    Evaluate the overall performance of a Part-of-Speech (PoS) tagger on a corpus.\n",
    "\n",
    "    This function computes:\n",
    "    - Sentence-level accuracy (how many entire sentences were perfectly tagged)\n",
    "    - Word-level accuracy (percentage of correctly predicted PoS tags)\n",
    "    - Per-label accuracy (how many times each PoS label was correctly predicted)\n",
    "\n",
    "    It relies on the helper functions:\n",
    "    `sentence_accuracy()`, `micro_word_accuracy()`, and `macro_word_accuracy()`.\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "    poslist_list : list[list[str]]\n",
    "        A list of lists of predicted PoS tags for each sentence in the corpus.\n",
    "\n",
    "    dico : list[dict]\n",
    "        The gold-standard corpus containing dictionaries of tokens and their\n",
    "        reference PoS tags, for example:\n",
    "        [\n",
    "            {'tokens': ['Je', 'mange', 'une', 'pomme'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']},\n",
    "            {'tokens': ['Tu', 'cours'], 'pos': ['PRON', 'VERB']}\n",
    "        ]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (sentence_accuracy_count, word_accuracy_percentage, per_label_accuracy_dict)\n",
    "        where:\n",
    "        - sentence_accuracy_count (int): number of perfectly predicted sentences\n",
    "        - word_accuracy_percentage (float): percentage of correct PoS tags across all sentences\n",
    "        - per_label_accuracy_dict (dict): number of correct predictions per PoS tag (e.g., {'VERB': 40, 'NOUN': 30, ...})\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> pos_pred = [\n",
    "    ...     ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ...     ['PRON', 'AUX', 'VERB', 'NOUN']\n",
    "    ... ]\n",
    "    >>> gold_data = [\n",
    "    ...     {'tokens': ['Je', 'mange', 'une', 'pomme'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']},\n",
    "    ...     {'tokens': ['Tu', 'as', 'mangé', 'pain'], 'pos': ['PRON', 'AUX', 'VERB', 'NOUN']}\n",
    "    ... ]\n",
    "    >>> postagger_accuracy(pos_pred, gold_data)\n",
    "    (2, 100.0, {'PRON': 2, 'VERB': 2, 'DET': 1, 'NOUN': 2, 'AUX': 1})\n",
    "    \n",
    "    Explanation\n",
    "    -----------\n",
    "    Both sentences were perfectly tagged → 2 correct sentences (100% accuracy).\n",
    "    The per-label dictionary shows how many PoS tags were correctly predicted in total.\n",
    "    \"\"\"\n",
    "    sentence=0\n",
    "    proportion=0\n",
    "    lengthtexte=0\n",
    "    correctpos={}\n",
    "    \n",
    "    for i in range(len(poslist_list)):\n",
    "        sentence=sentence+sentence_accuracy(poslist_list[i],dico[i])[1]\n",
    "        proportion= proportion+micro_word_accuracy(poslist_list[i],dico[i])\n",
    "        goldpos=dico[i].get(\"pos\")\n",
    "        lengthgoldpos=len(goldpos)\n",
    "        lengthtexte=lengthtexte+lengthgoldpos\n",
    "        correctpos=macro_word_accuracy(poslist_list[i],dico[i],correctpos)#update the correctpos by adding new keys if necessary or updating the key value\n",
    "    proportion=(proportion/lengthtexte)*100\n",
    "    return sentence,proportion,correctpos\n",
    "\"\"\"The Three following lines aren't usefull they are here to test postagger_accuracy quickly on the following arguments \"\"\"\n",
    "pos_pred = [['PRON', 'VERB', 'DET', 'NOUN'],['PRON', 'AUX', 'VERB', 'NOUN']]\n",
    "gold_data = [ {'tokens': ['Je', 'mange', 'une', 'pomme'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']},{'tokens': ['Tu', 'as', 'mangé', 'pain'], 'pos': ['PRON', 'AUX', 'VERB', 'NOUN']}]\n",
    "#postagger_accuracy(pos_pred, gold_data))   \n",
    "        \n",
    "def word_error_rate(poslist_list, dico):\n",
    "    \"\"\"\n",
    "    Compute the Word Error Rate (WER) for a PoS tagger on a corpus.\n",
    "\n",
    "    This metric measures the percentage of incorrectly predicted PoS tags\n",
    "    relative to the total number of tags in the gold-standard data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    poslist_list : list[list[str]]\n",
    "        A list of lists of predicted PoS tags for each sentence.\n",
    "\n",
    "    dico : list[dict]\n",
    "        The gold-standard corpus containing dictionaries with tokens and their true PoS tags.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The Word Error Rate (WER) expressed as a percentage of incorrect PoS predictions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> pos_pred = [\n",
    "    ...     ['PRON', 'VERB', 'DET', 'NOUN'],\n",
    "    ...     ['PRON', 'AUX', 'VERB', 'NOUN']\n",
    "    ... ]\n",
    "    >>> gold_data = [\n",
    "    ...     {'tokens': ['Je', 'mange', 'une', 'pomme'], 'pos': ['PRON', 'VERB', 'DET', 'NOUN']},\n",
    "    ...     {'tokens': ['Tu', 'as', 'mangé', 'pain'], 'pos': ['PRON', 'AUX', 'VERB', 'NOUN']}\n",
    "    ... ]\n",
    "    >>> word_error_rate(pos_pred, gold_data)\n",
    "    0\n",
    "\n",
    "    Explanation\n",
    "    -----------\n",
    "    All PoS tags are correct → 0% error rate.\n",
    "    If one tag had been wrong (e.g. VERB predicted as ADJ), \n",
    "    the function would return a non-zero WER.\n",
    "    \"\"\"\n",
    "    word_accurate=0\n",
    "    lengthtexte=0\n",
    "    for i in range(len(poslist_list)):\n",
    "        word_accurate=word_accurate+micro_word_accuracy(poslist_list[i],dico[i])\n",
    "        goldpos=dico[i].get(\"pos\")\n",
    "        lengthgoldpos=len(goldpos)\n",
    "        lengthtexte=lengthtexte+lengthgoldpos\n",
    "    word_error_rate_value=((lengthtexte-word_accurate)/lengthtexte)*100\n",
    "    return (int(word_error_rate_value))\n",
    "\n",
    "def corpus_stats(filepath):\n",
    "    \"\"\"\n",
    "    Extract and return all sentences (tokens only) from a JSON corpus as a single string.\n",
    "\n",
    "    The function reads a JSON file containing a list of sentence dictionaries,\n",
    "    each with a 'tokens' key. It concatenates all tokens into complete sentences,\n",
    "    joins them with spaces, and finally returns a string containing the entire corpus.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the JSON corpus file.\n",
    "                        The file should contain a list of dictionaries with a 'tokens' key.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all sentences from the corpus,\n",
    "             with tokens joined by spaces.\n",
    "\n",
    "    Example:\n",
    "        >>> text = corpus_tokens_only(\"data/sequoia_sample.json\")\n",
    "        >>> print(text[:100])\n",
    "        'Je mange une pomme . Tu adores le chocolat . Il boit du café ...'\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = json.load(f)\n",
    "\n",
    "    # Nombre de lignes = nombre d'entrées dans la liste\n",
    "    nb_lignes = len(corpus)\n",
    "    nb_mots=0 # initialize varaible at 0\n",
    "    listkey=[]\n",
    "    i=0\n",
    "    while i<len(corpus):# this whole loop must be changed because it is not straigt to the point enough\n",
    "        for k in corpus[i]:\n",
    "            listemots=corpus[i][k]\n",
    "            nb_mots=nb_mots+len(listemots)# not efficient becuase count pos and it is useless but works\n",
    "        i=i+1\n",
    "    nb_mots=int(nb_mots/2) # divides by 2 to retrieve the Pos labels of the number of words\n",
    "    \n",
    "    print(f\"Nombre de phrases : {nb_lignes}\")\n",
    "    print(f\"Nombre total de mots : {nb_mots}\")\n",
    "\n",
    "    return nb_lignes, nb_mots\n",
    "\n",
    "def corpus_tokens_only(filepath):\n",
    "    \"\"\"\n",
    "    Extract and return all sentences (tokens only) from a JSON corpus as a single string.\n",
    "\n",
    "    The function reads a JSON file containing a list of sentence dictionaries,\n",
    "    each with a 'tokens' key. It concatenates all tokens into complete sentences,\n",
    "    joins them with spaces, and finally returns a string containing the entire corpus.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the JSON corpus file.\n",
    "                        The file should contain a list of dictionaries with a 'tokens' key.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all sentences from the corpus,\n",
    "             with tokens joined by spaces.\n",
    "\n",
    "    Example:\n",
    "        >>> text = corpus_tokens_only(\"data/sequoia_sample.json\")\n",
    "        >>> print(text[:100])\n",
    "        'Je mange une pomme . Tu adores le chocolat . Il boit du café ...'\n",
    "        \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = json.load(f)\n",
    "    liste_phrase=[]\n",
    "    i=0\n",
    "    while i<len(corpus):# this whole loop must be changed\n",
    "        for k in corpus[i]:\n",
    "            if k==\"tokens\":\n",
    "                liste_phrase.append(\" \".join(corpus[i][k]))\n",
    "        i=i+1\n",
    "    str_corpus=\"\".join(liste_phrase)\n",
    "    return str_corpus\n",
    "       \n",
    "                \n",
    "corpusb=corpus_tokens_only(\"sequoia.test.json\")# list of tokenize sentences from the sequoia file\n",
    "lpos=predict_pos_list(corpusb, nlp)\n",
    "print(\"Test de WER sur le courpus Sequoia:\")\n",
    "print(word_error_rate(lpos, goalpos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f856afd-3dab-487e-8ef0-12dea9e73349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scikit-learn)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
